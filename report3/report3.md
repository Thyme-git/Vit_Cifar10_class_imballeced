# æ·±åº¦å­¦ä¹ ä¸è®¡ç®—æœºè§†è§‰å®éªŒä¸‰ï¼š CIFAR-ViT

<center>
<div class="is-size-5 publication-authors">
<span class="author-block">
  <b style="font-size: 20px;">å®æ¯“ä¼Ÿ</b><sup></sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span>
<span class="author-block">
  <b style="font-size: 20px;">æ¨è¿›å²³</b><sup>*</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<span class="author-block">
  <b style="font-size: 20px;">å¼ å­é™†</b><sup>*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</span>
<span class="author-block">
  <b style="font-size: 20px;">å¼ åœ£æƒ</b><sup>*</sup>
</span>
</div>
</center>
<center>
<div>
<p style="font-size: 16px;"><sup>*</sup>è¡¨ç¤ºåŒç­‰è´¡çŒ®</p>
</div>
</center>

> åˆ†å·¥ï¼š
> - å®æ¯“ä¼Ÿ(U202115325)ï¼šå†™ä»£ç ã€åšå®éªŒã€å†™æŠ¥å‘Šã€‚
> - æ¨è¿›å²³(U202114049)ï¼šå†™ä¸€ç‚¹ä»£ç ã€‚
> - å¼ å­é™†(U202115070)ï¼šåšä¸€ç‚¹å®éªŒã€‚
> - å¼ åœ£æƒ(U202112179)ï¼šå†™ä¸€ç‚¹æŠ¥å‘Šã€‚
> 
> å·¥ä½œé‡ä¸å¥½é‡åŒ–ï¼Œå› æ­¤ç¡®åˆ‡çš„ç»„å†…è´¡çŒ®æ¯”ä¾‹æ— æ³•ç»™å‡ºã€‚æœ›è€å¸ˆæµ·æ¶µã€‚

## Vision Transformerç½‘ç»œçš„ä»‹ç»ä¸å®ç°ï¼ˆè¦æ±‚2.1ï¼Œå¿…åšéƒ¨åˆ†ï¼‰

> ç›¸å…³ä»£ç ä½äº ``vit.py`` æ–‡ä»¶ä¸­ã€‚

Vision Transformer å…±åŒ…å«æœ‰ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ¨¡å—ï¼š

1. Multi-head Self-Attention (MSA)
2. Feed Forward Network (FFN)
3. Patch Embedding
4. Normalization Layer

[ğŸ’¡]æœ¬æ–‡çš„å®ç°ä¸æ–‡ç«  *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale* ä¸­çš„å®ç°æœ‰æ‰€ä¸åŒï¼š
  1. æœ¬æ–‡å¹¶æœªä½¿ç”¨ Class token, è¿™ä¸€ç‚¹ä¸å…¶åŒæ—¶æœŸçš„å¦ä¸€ç¯‡æ–‡ç«  *Swin Transformer: Hierarchical Vision Transformer using Shifted Windows* ä¿æŒä¸€è‡´ã€‚
  2. æœ¬æ–‡ä½¿ç”¨çš„Normalization Layeræ˜¯BatchNorm2dï¼Œè€Œä¸æ˜¯LayerNormã€‚


åœ¨æœ¬ç« èŠ‚ä¸­ï¼Œæˆ‘ä¼šè¯¦ç»†ä»‹ç»è¿™å‡ ä¸ªæ¨¡å—çš„å®ç°ã€‚

### Multi-head Self-Attention (MSA)

Self-Attention æ˜¯ Transformer ä¸­çš„æ ¸å¿ƒæ¨¡å—ï¼Œå®ƒèƒ½å¤Ÿæ•æ‰è¾“å…¥åºåˆ—ä¸­ä¸åŒä½ç½®ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚Attention çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

å…¶ä¸­ $Q, K, V$ åˆ†åˆ«ä»£è¡¨ Query, Key, Valueï¼Œ$d_k$ ä»£è¡¨ Key çš„ç»´åº¦ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°† $Q, K, V$ åˆ†åˆ«é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ° $Q', K', V'$ï¼Œç„¶åå†è¿›è¡Œ Attention çš„è®¡ç®—ã€‚å¯¹äº Multi-head Self-Attentionï¼Œæˆ‘ä»¬ä¼šå°† $Q, K, V$ åˆ†åˆ«é€šè¿‡ $h$ ä¸ªçº¿æ€§å˜æ¢å¾—åˆ° $Q_i, K_i, V_i$ï¼Œç„¶åå°† $h$ ä¸ª Attention çš„ç»“æœæ‹¼æ¥èµ·æ¥ï¼Œå†é€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

```python
class Attention(nn.Module):
    """
    Attention for images
    Input:
        - x: (B, C, H, W), already patched
    Output:
        - x: (B, C, H, W)
    """
    def __init__(
            self,
            in_channels: int,
            num_heads: int,
            dropout: float = 0.0
        ):
        assert in_channels % num_heads == 0, \
            f"in_channels(got {in_channels}) must be divisible by num_heads(got {num_heads})"

        super().__init__()
        self.qkv_transform = nn.Conv2d(
            in_channels=in_channels,
            out_channels=in_channels * 3,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=False,
        )
        # self.dropout = nn.Dropout(dropout)
        self.dropout = dropout
        self.softmax = nn.Softmax(dim=-1)
        self.num_heads = num_heads
        self.scale = in_channels ** -0.5
    
        self._init_weights()
    
    def _init_weights(self):
        nn.init.xavier_uniform_(self.qkv_transform.weight)

    def drop_attn(self, attn):
        B, h, N, _ = attn.shape
        mask = torch.rand((B, h, N, N), device=attn.device) < self.dropout
        attn[mask] = float('-inf')
        return attn

    def forward(self, x):
        B, C, H, W = x.shape
        qkv = self.qkv_transform(x)
        q, k, v = torch.chunk(qkv, 3, dim=1)
        q, k, v = map(lambda t: rearrange(t, 'B (h d) H W -> B h (H W) d', h=self.num_heads), (q, k, v))
        
        attn = (q @ k.transpose(-2, -1)) * self.scale # (B, h, H*W, H*W)
        attn = self.drop_attn(attn)
        x = self.softmax(attn) @ v
        x = rearrange(x, 'B h (H W) d -> B (h d) H W', H=H, W=W)
        return x
```

> ä½¿ç”¨äº†å·ç§¯å®ç°Q K Vçš„çº¿æ€§å˜æ¢ã€‚

### Feed Forward Network (FFN)

Feed Forward Network å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ã€‚å®ƒå¯¹æ¯ä¸ªä½ç½®çš„ç‰¹å¾å‘é‡è¿›è¡Œç›¸åŒçš„å˜åŒ–ï¼Œå› æ­¤æ˜¯ä¸€ä¸ªfeature-wiseçš„æ“ä½œã€‚

```python
class FFN(nn.Module):
    """
    Feed Forward Network for images
    Input:
        - x: (B, C, H, W)
    Output:
        - x: (B, C, H, W)
    """
    def __init__(
            self,
            in_channels: int,
            hidden_channels: int,
            dropout: float = 0.0
        ):
        super().__init__()
        self.ffn = nn.Sequential(
            nn.Conv2d(in_channels, hidden_channels, kernel_size=1),
            nn.GELU(),
            nn.Dropout(dropout),

            nn.Conv2d(hidden_channels, in_channels, kernel_size=1),
            nn.Dropout(dropout),
        )
    
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.xavier_uniform_(m.weight)

    def forward(self, x):
        return self.ffn(x)
```

### Patch Embedding

Patch Embedding æ˜¯å°†å›¾åƒåˆ†å‰²æˆå¤šä¸ª patchï¼Œå¹¶å°†æ¯ä¸ª patch è½¬æ¢æˆä¸€ä¸ªç‰¹å¾å‘é‡ã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥é€šè¿‡ä¸€ä¸ªå·ç§¯å±‚æ¥å®ç°ã€‚

```python
class VisionTransformer(nn.Mudule):
    def __init__(self, ...):
        self.patchify = nn.Conv2d(
            in_channels=in_channels,
            out_channels=d_model,
            kernel_size=patch_size,
            stride=patch_size,
            padding=0,
            bias=True,
        )
```

## ç±»ä¸å‡è¡¡ç°è±¡çš„è§£å†³æ–¹æ¡ˆï¼ˆè¦æ±‚2.2ï¼Œå¿…åšéƒ¨åˆ†ï¼‰

### Self-supervised pre-training ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼Œä¸æ·»åŠ æ–°æ•°æ®ï¼‰

æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç®€å•çš„è‡ªç›‘ç£è®­ç»ƒç¼–è§£ç å™¨æ¨¡å—ï¼Œå…¶ä¸­ç¼–ç å™¨ä¸ºVitæ¶æ„ï¼Œè€Œè§£ç å™¨åˆ™ä½¿ç”¨äº†å·ç§¯å’Œåå·ç§¯ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å°†ç¼–è§£ç å™¨æ¨¡å‹åœ¨è®­ç»ƒé›† ``CIFAR10_imbalanced`` è¿›è¡Œè‡ªç›‘ç£è®­ç»ƒï¼Œç„¶åå°†ç¼–ç å™¨çš„å‚æ•°è¿ç§»åˆ°åˆ†ç±»ä»»åŠ¡ä¸­ã€‚

å°† ``VisionTransformer`` çš„ç»“æ„åšå¦‚ä¸‹ä¿®æ”¹ï¼š 
```python
self.header = nn.Sequential(
    nn.Flatten(),
    nn.Linear(d_model * H * W, (d_model*H*W) // 2),
    nn.GELU(),
    nn.Linear((d_model*H*W) // 2, num_classes),
) if classifier else nn.Sequential(
    nn.ConvTranspose2d(d_model, in_channels,
        kernel_size=patch_size, stride=patch_size),
    nn.GELU(),
    nn.Conv2d(in_channels, in_channels,
        kernel_size=3, padding=1, stride=1),
    nn.Tanh(),
)
```
å³å¯å¾—åˆ°ä¸€ä¸ªç®€å•çš„è‡ªç›‘ç£ç¼–ç å™¨ã€‚

å½“æˆ‘ä»¬è®­ç»ƒå¥½ç¼–ç å™¨åï¼Œæˆ‘ä»¬å°†å…¶å‚æ•°è¿ç§»åˆ°åˆ†ç±»ä»»åŠ¡ä¸­ã€‚åœ¨å¾®è°ƒçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ç¼–ç å™¨å‚æ•°çš„å­¦ä¹ ç‡è®¾ç½®ä¸ºåŸæ¥çš„ 1/100ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

### Data Augmentation

æ•°æ®å¢å¼ºå¯¹äºç»å¤§éƒ¨åˆ†æ·±åº¦å­¦ä¹ ä»»åŠ¡æ¥è¯´ï¼Œå¯ä»¥åšåˆ°é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ•ˆæœã€‚ä½†æ˜¯åœ¨æœ¬å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°ï¼Œè¿‡åˆ†çš„æ•°æ®å¢å¼ºä¼šå¯¼è‡´æ›´å¤§çš„è¿‡æ‹Ÿåˆç¨‹åº¦ã€‚å¯¹æ­¤ï¼Œæˆ‘ä»¬çš„è§£é‡Šæ˜¯ï¼š è®­ç»ƒé›† ``CIFAR10_imbalanced`` å’Œæµ‹è¯•é›† ``CIFAR10_balance`` çš„åˆ†å¸ƒå·®å¼‚è¿‡å¤§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°å¹¶ä¸èƒ½å¾ˆå¥½çš„æ³›åŒ–åˆ°æµ‹è¯•é›†ä¸Šã€‚

å› æ­¤ï¼Œåœ¨æœ¬å®éªŒä¸­ï¼Œæˆ‘ä»¬åªé€‰ç”¨äº†ä¸¤ç§ç®€å•çš„æ•°æ®å¢å¼ºæ–¹å¼ï¼šRandomHorizontalFlip å’Œ RandomCropã€‚
```python
self.tf = transforms.Compose([
    transforms.Normalize(mean=(0.5, 0.5, 0.5),
                    std=(0.5, 0.5, 0.5)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0),
                    ratio=(0.9, 1.1), antialias=None),
])
```
æˆ‘ä»¬è¿˜å‘ç°ï¼Œå¦‚æœä½¿ç”¨äº† ColorJitter è¿™æ ·çš„æ•°æ®å¢å¼ºæ–¹å¼ï¼Œä¼šå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°æ›´å·®ã€‚

### Class Weighted Cross Entropy Loss

ç”±äºè®­ç»ƒé›† ``CIFAR10_imbalanced`` ä¸­çš„ç±»åˆ«åˆ†å¸ƒä¸å‡è¡¡ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ›´åŠ åå‘äºä¼˜åŒ–æ•°é‡è¾ƒå¤šçš„ç±»åˆ«ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°† Cross Entropy Loss çš„æƒé‡è®¾ç½®ä¸ºå„ä¸ªç±»åˆ«çš„å€’æ•°ã€‚å…·ä½“è€Œè¨€ï¼Œå¯¹äºç±»åˆ«ä¸º $i$ çš„æ ·æœ¬ï¼Œå…¶æƒé‡ä¸º $N \over {N_i}$ã€‚å…¶ä¸­ $N$ ä¸ºæ€»æ ·æœ¬æ•°ï¼Œ$N_i$ ä¸ºç±»åˆ« $i$ çš„æ ·æœ¬æ•°ã€‚
```python
cls_weight = torch.tensor([v for (_, v) in train_dataset.cls_cnt])
cls_weight = cls_weight.sum() / cls_weight
cls_weight = cls_weight / cls_weight.sum()
ce_loss_fn = torch.nn.CrossEntropyLoss(cls_weight.to(train_cfg.device))
```

### Online Hard Example Mining

è¿™ä¸ªåå­—ä¸€å¬èµ·æ¥å°±å¾ˆé«˜å¤§ä¸Šï¼Œä½†å®é™…ä¸Šå°±æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç­›é€‰éƒ¨åˆ†æ¯”è¾ƒéš¾åˆ†ç±»çš„æ ·æœ¬ï¼Œå¹¶ä»…ä»…ä½¿ç”¨è¿™éƒ¨åˆ†æ ·æœ¬è¿›è¡Œåå‘ä¼ æ’­ï¼Œä¼˜åŒ–ç½‘ç»œå‚æ•°ã€‚é‚£ä¹ˆæ€ä¹ˆé€‰å‡ºè¿™éƒ¨åˆ†æ ·æœ¬å‘¢ï¼Ÿå¯¹äºåˆ†ç±»é—®é¢˜æ¥è¯´ï¼Œåªéœ€è¦åˆ¤æ–­æŸä¸ªæ ·æœ¬çš„åˆ†ç±»æŸå¤±å³å¯ã€‚ä»£ç å¦‚ä¸‹ï¼š
```python
class OHEM_CELoss(nn.Module):
    def __init__(self, ratio=0.5, **kwargs):
        super().__init__()
        self.ratio = ratio
        self.loss_fn = nn.CrossEntropyLoss(reduction='none', **kwargs)

    def forward(self, pred, target):
        loss = self.loss_fn(pred, target)
        num = int(self.ratio * loss.size(0))
        loss, _ = loss.topk(num)
        return loss.mean()
```

### æ•´ä½“æ•ˆæœ

| Vanilla Algorithm | Improved Algorithm |
|:------------------:|:-------------------:|
| ![vanilla_alg](./image/report3/vanilla_alg.png) | ![improved_alg](./image/report3/improved.png) |

[ğŸ’¡]å¯¹æ•ˆæœæå‡æœ€æ˜æ˜¾çš„æ”¹å–„æ–¹æ³•å®é™…ä¸Šæ˜¯ Self-Supvised Pre-trainingï¼Œå¯èƒ½æ˜¯å› ä¸ºè‡ªç›‘ç£çš„è¿‡ç¨‹å®é™…ä¸Šæ˜¯ class-free çš„ã€‚åŒæ—¶ï¼ŒVitæ¡†æ¶ä¸‹çš„è‡ªç›‘ç£è®­ç»ƒå¯ä»¥è®©æ¨¡å‹å­¦ä¼šåŒºåˆ†ä¸åŒçš„â€œå­å›¾â€ï¼Œè¿™æ—¶å› ä¸ºAttentionä½œä¸ºå…¶æ ¸å¿ƒæ¨¡å—ï¼Œä»æœ¬è´¨ä¸Šæ¥è¯´å®åœ¨è®¡ç®—åŒä¸€å¼ å›¾åƒä¸åŒå­å›¾ä¹‹é—´çš„è‡ªç›¸å…³å…³ç³»ã€‚ç›¸ä¼¼æ€§å¤§çš„å­å›¾ï¼Œå…¶ç‰¹å¾å‘é‡çš„å¤¹è§’ä¼šæ¯”è¾ƒå°ï¼Œç›¸ä¼¼æ€§å°çš„å­å›¾ï¼Œå…¶ç‰¹å¾å‘é‡çš„å¤¹è§’ä¼šæ¯”è¾ƒå¤§ã€‚è‡ªç›‘ç£çš„å­¦ä¹ å¯ä»¥è®©æ¨¡å‹å­¦ä¼šåŒºåˆ†ä¸åŒçš„å›¾åƒéƒ¨åˆ†ï¼Œè¿™ä¸è‡ªç„¶è¯­è¨€å¤„ç†çš„é¢„è®­ç»ƒæœ‰ç€å¼‚æ›²åŒå·¥ä¹‹å¦™ã€‚


## åˆ†æViTä¸åŒæ¨¡å—å¯¹åˆ†ç±»ç»“æœçš„å½±å“ï¼ˆè¦æ±‚2.3ï¼Œå¿…åšéƒ¨åˆ†ï¼‰


### é»˜è®¤å®éªŒè®¾ç½®

ä¸ºäº†æ–¹ä¾¿æè¿°ï¼Œä»¥ä¸‹æ¯ä¸ªå®éªŒä»…æ”¹å˜ä¸€ä¸ªè¶…å‚æ•°ï¼Œå…¶ä»–è¶…å‚æ•°ä¿æŒä¸å˜ã€‚é»˜è®¤å®éªŒè®¾ç½®å¦‚ä¸‹ï¼š

```python
@dataclass
class ModelConfig:
    """Vit Model configuration"""
    num_classes: int = 10
    in_channels: int = 3
    img_size: tuple[int, int] = (32, 32)
    patch_size: int = 4
    d_model: int = 256
    num_heads: int = 4
    num_layers: int = 6
    ffn_hidden_channels: int = 512
    dropout: float = 0.1
    classifier: bool = True
```


### Patch Sizeçš„å½±å“

æˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨äº† ``patch_size=4`` ã€ ``patch_size=8`` å’Œ ``patch_size=16`` ä¸‰ç§ä¸åŒçš„ patch size è¿›è¡Œå®éªŒï¼Œå¾—åˆ°äº†å¦‚ä¸‹çš„ç»“æœï¼š

| Patch Size = 4 | Patch Size = 8 |
|:--------------:|:--------------:|
| ![ps_4](./image/report3/ps_4.png) | ![ps_8](./image/report3/ps_8.png) |

| Patch Size = 16 |
|:--------------:|
![ps_16](./image/report3/ps_16.png)

å¯ä»¥çœ‹åˆ°ï¼Œå½“ Patch Size ä¸º 16 æ—¶ï¼Œæ¨¡å‹çš„è¡¨ç°éª¤é™ï¼Œè¿™æ˜¯å› ä¸º Patch Size è¿‡å¤§ï¼Œå¯¼è‡´äº†æ¨¡å‹æ— æ³•æ•æ‰åˆ°å›¾åƒä¸­çš„ç»†èŠ‚ä¿¡æ¯ã€‚

### Embedding Dimension çš„å½±å“

æˆ‘ä»¬åˆ†åˆ«å°è¯•äº† ``d_model=64`` ã€ ``d_model=128`` ã€ ``d_model=256`` å’Œ ``d_model=512`` å››ç§ä¸åŒçš„ embedding dimension è¿›è¡Œå®éªŒï¼Œå¾—åˆ°äº†å¦‚ä¸‹çš„ç»“æœï¼š

| d_model = 64 | d_model = 128 |
|:-------------:|:-------------:|
| ![d_128](./image/report3/d_64.png) | ![d_256](./image/report3/d_128.png) |

| d_model = 256 | d_model = 512 |
|:-------------:|:-------------:|
| ![d_512](./image/report3/d_256.png) | ![d_1024](./image/report3/d_512.png) |

å¯ä»¥çœ‹åˆ°ï¼Œéšç€ embedding dimension çš„å¢å¤§ï¼Œæ¨¡å‹çš„è¡¨ç°é€æ¸æå‡ã€‚ååˆ†å¯æƒœçš„æ˜¯ï¼Œæˆ‘åªæœ‰ 24G çš„æ˜¾å­˜èµ„æºï¼Œæ— æ³•å°è¯•æ›´å¤§çš„ embedding dimensionã€‚

### Number of Heads çš„å½±å“

æˆ‘ä»¬åˆ†åˆ«å°è¯•äº† ``num_heads=2`` ã€ ``num_heads=4`` ã€ ``num_heads=8`` å’Œ ``num_heads=16`` å››ç§ä¸åŒçš„ head æ•°ç›®è¿›è¡Œå®éªŒï¼Œå¾—åˆ°äº†å¦‚ä¸‹çš„ç»“æœï¼š

| num_heads = 2 | num_heads = 4 |
|:-------------:|:-------------:|
| ![nh_2](./image/report3/nh_2.png) | ![nh_4](./image/report3/nh_4.png) |

| num_heads = 8 | num_heads = 16 |
|:-------------:|:-------------:|
| ![nh_8](./image/report3/nh_8.png) | ![nh_16](./image/report3/nh_16.png) |

å¯ä»¥å‘ç°ï¼Œåœ¨ num_heads = 8 æ—¶ï¼Œæ¨¡å‹çš„è¡¨ç°è¾¾åˆ°äº†æœ€å¥½ã€‚è¿‡å¤§/è¿‡å°çš„ num_heads éƒ½ä¼šå¯¼è‡´æ¨¡å‹çš„è¡¨ç°ä¸‹é™ã€‚


## ViTè½»é‡åŒ–ï¼ˆè¦æ±‚2.4ï¼Œé€‰åšéƒ¨åˆ†ï¼‰

ViTï¼ˆVision Transformerï¼‰è½»é‡åŒ–çš„ä¸»è¦ç›®æ ‡æ˜¯å‡å°‘æ¨¡å‹çš„å‚æ•°å’Œè®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚é€šè¿‡æŠ€æœ¯å¦‚çŸ¥è¯†è’¸é¦ã€æƒé‡å‰ªæå’Œæ¨¡å‹é‡åŒ–ï¼ŒViTå¯ä»¥åœ¨ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ä¸­æ›´æœ‰æ•ˆåœ°è¿è¡Œã€‚è½»é‡åŒ–ç‰ˆæœ¬é€šå¸¸é‡‡ç”¨æ›´å°çš„è¾“å…¥åˆ†è¾¨ç‡å’Œå‡å°‘å±‚æ•°ï¼Œç¡®ä¿åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä»èƒ½æä¾›å‡ºè‰²çš„è§†è§‰è¯†åˆ«èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•è®©ViTåœ¨å¤„ç†å¤æ‚å›¾åƒä»»åŠ¡æ—¶ï¼Œå…¼é¡¾æ•ˆç‡ä¸æ•ˆæœã€‚

xFormersæ˜¯ä¸€ä¸ªæ—¨åœ¨åŠ é€ŸTransformerç›¸å…³ç ”ç©¶çš„å·¥å…·ç®±ï¼Œæä¾›äº†ä¸€ç³»åˆ—å¯å®šåˆ¶çš„æ„å»ºæ¨¡å—ã€‚è¿™äº›æ¨¡å—ç‹¬ç«‹ä¸”å¯è‡ªå®šä¹‰ï¼Œæ— éœ€ç¹ççš„ä»£ç ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥æ–¹ä¾¿åœ°åœ¨è§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸä¸­åº”ç”¨ã€‚xFormerså…³æ³¨å‰æ²¿ç ”ç©¶ï¼ŒåŒ…å«è®¸å¤šå°šæœªåœ¨ä¸»æµåº“ï¼ˆå¦‚PyTorchï¼‰ä¸­å®ç°çš„æœ€æ–°ç»„ä»¶ã€‚æ­¤å¤–ï¼ŒxFormersç‰¹åˆ«æ³¨é‡æ•ˆç‡ï¼Œæ‰€æœ‰ç»„ä»¶éƒ½ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿å¿«é€Ÿçš„è¿­ä»£é€Ÿåº¦å’Œè‰¯å¥½çš„å†…å­˜åˆ©ç”¨ç‡ã€‚å®ƒè¿˜é›†æˆäº†è‡ªå®šä¹‰CUDAå†…æ ¸ï¼Œå¹¶åœ¨å¿…è¦æ—¶è°ƒç”¨å…¶ä»–åº“ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

